{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T11:56:54.184713800Z",
     "start_time": "2023-09-12T11:56:53.446871900Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data preprocessing\n",
    "- remove all columns except: 'modified_sequence', 'precursor_charge', 'precursor_intensity'\n",
    "- filter out unwanted charge states\n",
    "- filter for most abundant charge state per sequence by precursor_intensity after normalizing the values\n",
    "- filter sequence length according to occurance in dataset (currently less than 100 sequences of a certain length get removed)\n",
    "- search for occurences of UNIMOD modifications and add them to the vocabulary\n",
    "- generate continous sequence encoding // first layer - embedding layer\n",
    "- generate precursor_charge one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:56:59.982562600Z",
     "start_time": "2023-09-12T11:56:53.465326500Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Combine files into one dataframe and keep only desired columns\n",
    "Default: \n",
    "- dir_path = 'data/'\n",
    "- file_type = '.parquet'\n",
    "Default: drop everything except: modified_sequence, precursor_charge and precursor_intensity\n",
    "- columns_to_keep = ['modified_sequence','precursor_charge','precursor_intensity']\n",
    "'''\n",
    "def combine_parquet_into_df(dir_path='data/', file_type='.parquet', columns_to_keep=['modified_sequence','precursor_charge','precursor_intensity']):\n",
    "    dfs = [] \n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith(file_type):\n",
    "            file_path = os.path.join(dir_path, file)\n",
    "            df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "            df = df[columns_to_keep]\n",
    "            dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:05.337843300Z",
     "start_time": "2023-09-12T11:56:53.469362200Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Drop all rows with NaN values in a specific column\n",
    "Default: drop na from precursor_intensity column\n",
    "'''\n",
    "def drop_na(df, column='precursor_intensity'):\n",
    "    df = df[df[column].notna()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:07.295274800Z",
     "start_time": "2023-09-12T11:56:53.475754900Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Keep only desired charge entires\n",
    "Default: keep charges 1-6\n",
    "'''\n",
    "def keep_desired_charges(df, charge_list=[1, 2, 3, 4, 5, 6]):\n",
    "    df = df[df['precursor_charge'].isin(charge_list)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:07.336766500Z",
     "start_time": "2023-09-12T11:56:53.484646200Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Find all UNIMOD annotations and add them to the vocabulary\n",
    "(The length of the vocabulary +1 is used later for the embedding layer)\n",
    "'''\n",
    "def complete_vocabulary(df):\n",
    "    \"\"\"\n",
    "    Completes the vocabulary with all the possible amino acids and modifications\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    vocabulary = []\n",
    "    vocabulary+=list('XACDEFGHIKLMNPQRSTVWY')\n",
    "    annotations = re.findall(r'(\\w\\[UNIMOD:\\d+\\])', ' '.join(df['modified_sequence']))\n",
    "    for item in annotations:\n",
    "        if item not in vocabulary:\n",
    "                vocabulary.append(item)\n",
    "    vocab_len = len(vocabulary)\n",
    "    return vocabulary, vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:07.336766500Z",
     "start_time": "2023-09-12T11:56:53.492665500Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Combine unique sequences and aggregate their precursor_charges and intensity in order to later select the most abundant charge state per sequence.\n",
    "'''\n",
    "def aggregate_sequences(df):\n",
    "    df = df.groupby(\"modified_sequence\")[[\"precursor_charge\", \"precursor_intensity\"]].agg(list).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:07.336766500Z",
     "start_time": "2023-09-12T11:56:53.500199800Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: description\n",
    "'''\n",
    "Normalize precursor intensities for aggregated sequences\n",
    "'''\n",
    "def normalize_precursor_intensities(df_charge_list, df_intensity_list):\n",
    "    # Get the index of the most abundant precursor intensity\n",
    "    charge_dict = dict()\n",
    "    for index, i in enumerate(df_charge_list):\n",
    "        charge_dict[i] = []\n",
    "        charge_dict[i].append(df_intensity_list[index])\n",
    "\n",
    "    # Normalize the precursor intensity based on the most abundant precursor intensity\n",
    "    for key, value in charge_dict.items():\n",
    "        if len(value) > 1:\n",
    "            charge_dict[key] = sum(value) - min(value) / (max(value) - min(value))\n",
    "\n",
    "    # convert list of one float to float values\n",
    "    charge_dict = {key: value[0] for key, value in charge_dict.items()}\n",
    "\n",
    "    return charge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:07.336766500Z",
     "start_time": "2023-09-12T11:56:53.510861800Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: description\n",
    "'''\n",
    "Select most abundand charge state per unique sequence according to the normalized precursor intensity\n",
    "'''\n",
    "def get_most_abundant(df_charge_list, df_intensity_list, distributions=False):\n",
    "    charge_dict = dict()\n",
    "    for index, i in enumerate(df_charge_list):\n",
    "        if i not in charge_dict:\n",
    "            charge_dict[i] = df_intensity_list[index]\n",
    "        else:\n",
    "            charge_dict[i] += df_intensity_list[index]\n",
    "    if distributions:\n",
    "        return charge_dict\n",
    "    else:\n",
    "        return max(charge_dict, key=charge_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:07.623883400Z",
     "start_time": "2023-09-12T11:56:53.518698800Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "One-Hot encode most abundand charge state\n",
    "input: df with \"most_abundance_charge\" column\n",
    "output: new column \"most_abundant_charge_vector\" containing one-hot encoded vector\n",
    "'''\n",
    "def one_hot_encode_charge(df, charge_list=[1, 2, 3, 4, 5, 6]):\n",
    "    df['most_abundant_charge_vector'] = df['most_abundant_charge'].apply(lambda x: [1 if x == i else 0 for i in charge_list])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:09.223565300Z",
     "start_time": "2023-09-12T11:56:53.526452500Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: description\n",
    "'''\n",
    "Applying normalization, selecting most abundant charge state and one-hot encoding\n",
    "'''\n",
    "def normalize_and_select_most_abundant(df):\n",
    "    df['normalized'] = df.apply(lambda x: normalize_precursor_intensities(x[\"precursor_charge\"], x[\"precursor_intensity\"]), axis=1)\n",
    "    df['pre_normalization'] = df.apply(lambda x: get_most_abundant(x[\"precursor_charge\"], x[\"precursor_intensity\"], True), axis=1)\n",
    "    df['most_abundant_charge'] = df['normalized'].apply(lambda x: max(x, key=x.get))\n",
    "    df = one_hot_encode_charge(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:09.563547300Z",
     "start_time": "2023-09-12T11:56:53.533916500Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "get topK charge states for each sequence according to the normalized precursor intensity\n",
    "\n",
    "input: df with \"normalized\" column\n",
    "output: new column \"topK_charge_states\" containing list of topK charge states\n",
    "\n",
    "default: k=2\n",
    "'''\n",
    "def get_topK_charge_states(df, k=2):\n",
    "    def get_topK(label_dict):\n",
    "        allowed_keys = list()\n",
    "        sorted_values = sorted(label_dict.values(), reverse=True)\n",
    "        for i in sorted_values:\n",
    "            for key, value in label_dict.items():\n",
    "                if i == value and len(allowed_keys) <= k-1:\n",
    "                    allowed_keys.append(key)\n",
    "        return allowed_keys\n",
    "\n",
    "    df[f'top_{k}_charge_states'] = df['normalized'].apply(get_topK)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:10.210878500Z",
     "start_time": "2023-09-12T11:56:53.548156700Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Remove sequences of specific length represented less than a certain number of times\n",
    "\n",
    "input: df containig \"modified_sequence\" column, representation_threshold\n",
    "output: \n",
    "- df containing only sequence legths represented more than representation_threshold times\n",
    "- padding_length\n",
    "default: representation_threshold = 100\n",
    "\n",
    "Calculate the sequence lengths and their counts\n",
    "Filter out sequences with counts below the threshold\n",
    "Filter the original DataFrame based on sequence length\n",
    "Drop the temporary column\n",
    "'''\n",
    "def remove_rare_sequence_lengths(df, representation_threshold=100):\n",
    "    before_len = len(df)\n",
    "    df['sequence_length_prepadding'] = df['modified_sequence'].apply(len)\n",
    "    len_counts = df['sequence_length_prepadding'].value_counts().reset_index()\n",
    "    len_counts.columns = ['seq_len', 'count']\n",
    "    filtered_lengths = len_counts[len_counts['count'] >= representation_threshold]['seq_len']\n",
    "    df = df[df['sequence_length_prepadding'].isin(filtered_lengths)].copy()\n",
    "    padding_length = df['sequence_length_prepadding'].max()\n",
    "    df = df[df['sequence_length_prepadding'].isin(filtered_lengths)]\n",
    "    after_len = len(df)\n",
    "    print(f\"Removed {before_len - after_len} of {before_len} seqquences\")\n",
    "    return df, padding_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:10.260916700Z",
     "start_time": "2023-09-12T11:56:53.554125100Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Encode all occuring charge states per unique sequence in a binary vector\n",
    "\n",
    "input: df containing \"precursor_charge\" column\n",
    "output: df containing an additional \"charge_state_vector\" column encoding all occuring charge states per unique sequence in a binary vector\n",
    "'''\n",
    "def encode_charge_states(df):\n",
    "    df['charge_state_vector'] = df['precursor_charge'].apply(lambda x: [1 if i in x else 0 for i in range(1,7)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:10.264925Z",
     "start_time": "2023-09-12T11:56:53.562027600Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Checks if a vector contains only continous charge states e.g. [1,1,1,0,0,0]\n",
    "Flase if a vector contains skipped charges e.g. [1,0,0,0,0,1]\n",
    "\n",
    "input: charge_state_vector\n",
    "output: True if no charge state is skipped, False if a charge state is skipped\n",
    "'''\n",
    "def has_skipped_charges(charge_state_vector):\n",
    "    was_found = False\n",
    "    was_concluded = False\n",
    "    for i in charge_state_vector:\n",
    "        if i == 1 and not was_found:\n",
    "            was_found = True\n",
    "        if i == 0 and was_found:\n",
    "            was_concluded = True\n",
    "        if i == 1 and was_concluded:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:10.496685700Z",
     "start_time": "2023-09-12T11:56:53.638570200Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Filter out all sequences where has_skipped_charges() returns True\n",
    "\n",
    "input: df containing \"charge_state_vector\" column\n",
    "output: df containing only sequences where has_skipped_charges() returns False\n",
    "'''\n",
    "def filter_skipped_charges(df):\n",
    "    return df[df['charge_state_vector'].apply(lambda x: not has_skipped_charges(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T11:57:10.512664600Z",
     "start_time": "2023-09-12T11:56:53.643280100Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Removes sequences with skipped charges that occur less than a certain number of times\n",
    "\n",
    "input: df containing \"charge_state_vector\" column, cutoff\n",
    "output: df containing only sequences with skipped charges that occur more than cutoff times\n",
    "default: cutoff = 1000\n",
    "'''\n",
    "def skip_charges_for_occurrences(df, cutoff = 1000):\n",
    "    list_k = []\n",
    "    list_v = []\n",
    "    drop_out_index = []\n",
    "    for index, i in enumerate(df['charge_state_vector'].value_counts()):\n",
    "        list_k.append(df['charge_state_vector'].value_counts().index[index])\n",
    "        list_v.append(i)\n",
    "        if  has_skipped_charges(df['charge_state_vector'].value_counts().index[index]) and list_v[index] < cutoff:\n",
    "            drop_out_index.append(index)\n",
    "            \n",
    "    drop_out_list = []\n",
    "    for i in drop_out_index:\n",
    "        drop_out_list.append(list_k[i])\n",
    "    df_out = df[~df['charge_state_vector'].isin(drop_out_list)]\n",
    "    print(f\"Removed {len(df) - len(df_out)} of {len(df)} seqquences\")\n",
    "    return df_out    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T12:02:17.139713Z",
     "start_time": "2023-09-12T12:02:15.291892100Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encodes the 'modified_sequence' column in a DataFrame and adds a new column 'modified_sequence_vector'.\n",
    "\n",
    "input: df containing \"modified_sequence\" column, vocabulary, padding_length\n",
    "output: df containing \"modified_sequence_vector\" column with padded and encoded sequences\n",
    "\n",
    "defaults: padding_length = 50\n",
    "\"\"\"\n",
    "def sequence_encoder(df, padding_length=50, vocabulary=None):\n",
    "    \n",
    "    if 'modified_sequence' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'modified_sequence' column.\")\n",
    "\n",
    "    aa_dictionary = {aa: index for index, aa in enumerate(vocabulary)}\n",
    "\n",
    "    def encode_sequence(sequence):\n",
    "        pattern = r'[A-Z]\\[[^\\]]*\\]|.'\n",
    "        result = [match for match in re.findall(pattern, sequence)]\n",
    "        result += ['X'] * (padding_length - len(result))\n",
    "        return [aa_dictionary.get(aa, aa_dictionary['X']) for aa in result]\n",
    "\n",
    "    df['modified_sequence_vector'] = df['modified_sequence'].apply(encode_sequence)\n",
    "    print(\"Step 7/12 complete. Encoded all sequences.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T12:02:17.241110300Z",
     "start_time": "2023-09-12T12:02:15.297913600Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate overview plot for precursor_charge distribution in combined dataset\n",
    "'''\n",
    "def plot_most_abundant_charge_distribution(df):\n",
    "    # plot the distirbution of precursor_charge for the whole dataset\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "    ax = sns.countplot(x='most_abundant_charge', data=df, palette=\"viridis\")\n",
    "    plt.xlabel('Precursor Charge')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Precursor Charge')\n",
    "    # add percentage of each charge state to the plot\n",
    "    total = len(df['most_abundant_charge'])\n",
    "    for p in ax.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "        x = p.get_x() + p.get_width() / 2 - 0.05\n",
    "        y = p.get_y() + p.get_height() + 5\n",
    "        ax.annotate(percentage, (x, y))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T12:02:17.245108900Z",
     "start_time": "2023-09-12T12:02:15.307018400Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_topK_charge_distribution(df, column_name='top_2_charge_states'):\n",
    "    charge_state_counter = {\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0,\n",
    "        5: 0,\n",
    "        6: 0\n",
    "    }\n",
    "    \n",
    "    for row in df[column_name]:\n",
    "        for k in row:\n",
    "            charge_state_counter[k] = charge_state_counter[k] + 1\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "    palette = sns.color_palette(\"viridis\", len(charge_state_counter))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(len(charge_state_counter)), list(charge_state_counter.values()), align='center', color=palette)\n",
    "    plt.xticks(range(len(charge_state_counter)), list(charge_state_counter.keys()))\n",
    "    plt.xlabel('Charge State')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Charge State Distribution')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    total = len(df['most_abundant_charge'])\n",
    "    for p in plt.gca().patches:\n",
    "        height = p.get_height()\n",
    "        plt.gca().text(p.get_x() + p.get_width()/2., height + 3, '{:.1f}%'.format(height/total*100), ha='center', fontsize=12)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:02:26.594868700Z",
     "start_time": "2023-09-12T12:02:15.357679300Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_path = 'data/'\n",
    "file_type = '.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T12:02:40.118677Z",
     "start_time": "2023-09-12T12:02:15.360198500Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[228], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mcombine_parquet_into_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdir_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_type\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[206], line 14\u001B[0m, in \u001B[0;36mcombine_parquet_into_df\u001B[1;34m(dir_path, file_type, columns_to_keep)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file\u001B[38;5;241m.\u001B[39mendswith(file_type):\n\u001B[0;32m     13\u001B[0m     file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dir_path, file)\n\u001B[1;32m---> 14\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfastparquet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m     df \u001B[38;5;241m=\u001B[39m df[columns_to_keep]\n\u001B[0;32m     16\u001B[0m     dfs\u001B[38;5;241m.\u001B[39mappend(df)\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:503\u001B[0m, in \u001B[0;36mread_parquet\u001B[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001B[0m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001B[39;00m\n\u001B[0;32m    458\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    499\u001B[0m \u001B[38;5;124;03mDataFrame\u001B[39;00m\n\u001B[0;32m    500\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    501\u001B[0m impl \u001B[38;5;241m=\u001B[39m get_engine(engine)\n\u001B[1;32m--> 503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    505\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    506\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    507\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:358\u001B[0m, in \u001B[0;36mFastParquetImpl.read\u001B[1;34m(self, path, columns, storage_options, **kwargs)\u001B[0m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    357\u001B[0m     parquet_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mParquetFile(path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparquet_kwargs)\n\u001B[1;32m--> 358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparquet_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_pandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    360\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\fastparquet\\api.py:783\u001B[0m, in \u001B[0;36mParquetFile.to_pandas\u001B[1;34m(self, columns, categories, filters, index, row_filter, dtypes)\u001B[0m\n\u001B[0;32m    779\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    780\u001B[0m     parts \u001B[38;5;241m=\u001B[39m {name: (v \u001B[38;5;28;01mif\u001B[39;00m name\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-catdef\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    781\u001B[0m                     \u001B[38;5;28;01melse\u001B[39;00m v[start:start \u001B[38;5;241m+\u001B[39m thislen])\n\u001B[0;32m    782\u001B[0m              \u001B[38;5;28;01mfor\u001B[39;00m (name, v) \u001B[38;5;129;01min\u001B[39;00m views\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m--> 783\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_row_group_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcategories\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    784\u001B[0m \u001B[43m                             \u001B[49m\u001B[43massign\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartition_meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartition_meta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    785\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mrow_filter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfile\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    786\u001B[0m     start \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m thislen\n\u001B[0;32m    787\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\fastparquet\\api.py:385\u001B[0m, in \u001B[0;36mParquetFile.read_row_group_file\u001B[1;34m(self, rg, columns, categories, index, assign, partition_meta, row_filter, infile)\u001B[0m\n\u001B[0;32m    382\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    383\u001B[0m f \u001B[38;5;241m=\u001B[39m infile \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopen(fn, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 385\u001B[0m \u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_row_group\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcategories\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcats\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43mselfmade\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselfmade\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m    \u001B[49m\u001B[43massign\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheme\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile_scheme\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartition_meta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartition_meta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrow_filter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrow_filter\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret:\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\fastparquet\\core.py:631\u001B[0m, in \u001B[0;36mread_row_group\u001B[1;34m(file, rg, columns, categories, schema_helper, cats, selfmade, index, assign, scheme, partition_meta, row_filter)\u001B[0m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m assign \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGoing with pre-allocation!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 631\u001B[0m \u001B[43mread_row_group_arrays\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcategories\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema_helper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mcats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselfmade\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43massign\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow_filter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrow_filter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m cat \u001B[38;5;129;01min\u001B[39;00m cats:\n\u001B[0;32m    635\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cat \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m assign:\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;66;03m# do no need to have partition columns in output\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\fastparquet\\core.py:601\u001B[0m, in \u001B[0;36mread_row_group_arrays\u001B[1;34m(file, rg, columns, categories, schema_helper, cats, selfmade, assign, row_filter)\u001B[0m\n\u001B[0;32m    598\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    599\u001B[0m remains\u001B[38;5;241m.\u001B[39mdiscard(name)\n\u001B[1;32m--> 601\u001B[0m \u001B[43mread_col\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema_helper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m-catdef\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    602\u001B[0m \u001B[43m         \u001B[49m\u001B[43mselfmade\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mselfmade\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43massign\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    603\u001B[0m \u001B[43m         \u001B[49m\u001B[43mcatdef\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m-catdef\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    604\u001B[0m \u001B[43m         \u001B[49m\u001B[43mrow_filter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrow_filter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_map_like(schema_helper, column):\n\u001B[0;32m    607\u001B[0m     \u001B[38;5;66;03m# TODO: could be done in fast loop in _assemble_objects?\u001B[39;00m\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m maps:\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\fastparquet\\core.py:505\u001B[0m, in \u001B[0;36mread_col\u001B[1;34m(column, schema_helper, infile, use_cat, selfmade, assign, catdef, row_filter)\u001B[0m\n\u001B[0;32m    503\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    504\u001B[0m     skip_nulls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 505\u001B[0m defi, rep, val \u001B[38;5;241m=\u001B[39m \u001B[43mread_data_page\u001B[49m\u001B[43m(\u001B[49m\u001B[43minfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema_helper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcmd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    506\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mskip_nulls\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselfmade\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mselfmade\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    507\u001B[0m max_defi \u001B[38;5;241m=\u001B[39m schema_helper\u001B[38;5;241m.\u001B[39mmax_definition_level(cmd\u001B[38;5;241m.\u001B[39mpath_in_schema)\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(row_filter, np\u001B[38;5;241m.\u001B[39mndarray):\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\fastparquet\\core.py:124\u001B[0m, in \u001B[0;36mread_data_page\u001B[1;34m(f, helper, header, metadata, skip_nulls, selfmade)\u001B[0m\n\u001B[0;32m    122\u001B[0m     skip_definition_bytes(io_obj, daph\u001B[38;5;241m.\u001B[39mnum_values)\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 124\u001B[0m     definition_levels, num_nulls \u001B[38;5;241m=\u001B[39m \u001B[43mread_def\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdaph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhelper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    126\u001B[0m nval \u001B[38;5;241m=\u001B[39m daph\u001B[38;5;241m.\u001B[39mnum_values \u001B[38;5;241m-\u001B[39m num_nulls\n\u001B[0;32m    127\u001B[0m se \u001B[38;5;241m=\u001B[39m helper\u001B[38;5;241m.\u001B[39mschema_element(metadata\u001B[38;5;241m.\u001B[39mpath_in_schema)\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\fastparquet\\core.py:80\u001B[0m, in \u001B[0;36mread_def\u001B[1;34m(io_obj, daph, helper, metadata, out)\u001B[0m\n\u001B[0;32m     77\u001B[0m     num_nulls \u001B[38;5;241m=\u001B[39m metadata\u001B[38;5;241m.\u001B[39mstatistics\u001B[38;5;241m.\u001B[39mnull_count\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     79\u001B[0m     num_nulls \u001B[38;5;241m=\u001B[39m daph\u001B[38;5;241m.\u001B[39mnum_values \u001B[38;5;241m-\u001B[39m \u001B[43m(\u001B[49m\u001B[43mdefinition_levels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\n\u001B[1;32m---> 80\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43mmax_definition_level\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_nulls \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     82\u001B[0m     definition_levels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\numpy\\core\\_methods.py:46\u001B[0m, in \u001B[0;36m_sum\u001B[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_amin\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     43\u001B[0m           initial\u001B[38;5;241m=\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m umr_minimum(a, axis, \u001B[38;5;28;01mNone\u001B[39;00m, out, keepdims, initial, where)\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sum\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     47\u001B[0m          initial\u001B[38;5;241m=\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_prod\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     51\u001B[0m           initial\u001B[38;5;241m=\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "df = combine_parquet_into_df(dir_path, file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.976065300Z"
    }
   },
   "outputs": [],
   "source": [
    "df = drop_na(df, 'precursor_intensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.977573600Z"
    }
   },
   "outputs": [],
   "source": [
    "df = keep_desired_charges(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.978582100Z"
    }
   },
   "outputs": [],
   "source": [
    "df = aggregate_sequences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.979581100Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.981590200Z"
    }
   },
   "outputs": [],
   "source": [
    "df, padding_length = remove_rare_sequence_lengths(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.982589900Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.983589500Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary, voc_len = complete_vocabulary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.984590300Z"
    }
   },
   "outputs": [],
   "source": [
    "df = sequence_encoder(df, padding_length, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.985589Z"
    }
   },
   "outputs": [],
   "source": [
    "df = normalize_and_select_most_abundant(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.986588800Z"
    }
   },
   "outputs": [],
   "source": [
    "df = encode_charge_states(df)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.988103400Z"
    }
   },
   "outputs": [],
   "source": [
    "df = skip_charges_for_occurrences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.989112Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_most_abundant_charge_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.991126600Z"
    }
   },
   "outputs": [],
   "source": [
    "df = get_topK_charge_states(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.992127100Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_topK_charge_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dataset-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.993126100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.tail(4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.994126800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import timeit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:02:48.880023500Z",
     "start_time": "2023-09-12T12:02:48.449827500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:46:05.411565900Z",
     "start_time": "2023-09-12T12:46:04.932282600Z"
    }
   },
   "outputs": [],
   "source": [
    "class PrecursorChargeStateDataset:\n",
    "    def __init__(self, data_type=\"array\", classification_type=\"multi_class\", model_type=\"embedding\", charge_states=[1, 2, 3, 4, 5, 6], dir_path='data/', file_type='.parquet', columns_to_keep=['modified_sequence','precursor_charge','precursor_intensity']):\n",
    "        \n",
    "        \n",
    "        ''' CHECK ALL INPUTS '''\n",
    "        # check if data_type is valid\n",
    "        if isinstance(data_type, str):\n",
    "            if data_type not in [\"array\", \"dataframe\"]:\n",
    "                raise ValueError(\"data_type must be either 'array' or 'dataframe'.\")\n",
    "            else:\n",
    "                data_type = data_type.lower()\n",
    "        else:\n",
    "            raise TypeError(\"data_type must be a string.\")\n",
    "        \n",
    "        # check if classification_type is valid\n",
    "        if isinstance(classification_type, str):\n",
    "            if classification_type not in [\"multi_class\", \"multi_label\"]:\n",
    "                raise ValueError(\"classification_type must be either 'multi_class' or 'multi_label'.\")\n",
    "            else:\n",
    "                classification_type = classification_type.lower()\n",
    "        else:\n",
    "            raise TypeError(\"classification_type must be a string.\")\n",
    "        \n",
    "        # check if model_type is valid\n",
    "        if isinstance(model_type, str):\n",
    "            if model_type not in [\"embedding\", \"conv2d\", \"prosit\"]:\n",
    "                raise ValueError(\"model_type must be either 'embedding' or 'one_hot'.\")\n",
    "            else:\n",
    "                model_type = model_type.lower()\n",
    "        else:\n",
    "            raise TypeError(\"model_type must be a string.\")\n",
    "        \n",
    "        # check if charge states correct:\n",
    "        if isinstance(charge_states, list):\n",
    "            if not all(isinstance(item, int) for item in charge_states):\n",
    "                raise ValueError(\"charge_states must be a list of integers.\")\n",
    "        else:\n",
    "            raise TypeError(\"charge_states must be a list.\")\n",
    "        \n",
    "        # check dir_path\n",
    "        if isinstance(dir_path, str):\n",
    "            if not os.path.isdir(dir_path):\n",
    "                raise ValueError(\"dir_path must be a valid directory. Is not: {}\".format(dir_path))\n",
    "        else:\n",
    "            raise TypeError(\"dir_path must be a string.\")\n",
    "        \n",
    "        # check file_type\n",
    "        if isinstance(file_type, str):\n",
    "            if not file_type.startswith(\".\"):\n",
    "                file_type = \".\" + file_type\n",
    "        else:\n",
    "            raise TypeError(\"file_type must be a string.\")\n",
    "        \n",
    "        # check columns_to_keep\n",
    "        if isinstance(columns_to_keep, list):\n",
    "            if not all(isinstance(item, str) for item in columns_to_keep):\n",
    "                raise ValueError(\"columns_to_keep must be a list of strings. In Order: 'modified_sequence', 'precursor_charge', 'precursor_intensity'.\")\n",
    "        else:\n",
    "            raise TypeError(\"columns_to_keep must be a list.\")\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Combine files into one dataframe and keep only desired columns\n",
    "        Default: \n",
    "        - dir_path = 'data/'\n",
    "        - file_type = '.parquet'\n",
    "        Default: drop everything except: modified_sequence, precursor_charge and precursor_intensity\n",
    "        - columns_to_keep = ['modified_sequence','precursor_charge','precursor_intensity']\n",
    "        '''\n",
    "        def combine_parquet_into_df(dir_path='data/', file_type='.parquet', columns_to_keep=['modified_sequence','precursor_charge','precursor_intensity']):\n",
    "            dfs = [] \n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith(file_type):\n",
    "                    file_path = os.path.join(dir_path, file)\n",
    "                    df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "                    df = df[columns_to_keep]\n",
    "                    dfs.append(df)\n",
    "        \n",
    "            df = pd.concat(dfs, ignore_index=True)\n",
    "            print(f\"Step 1/12 complete. Combined {len(dfs)} files into one DataFrame.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Drop all rows with NaN values in a specific column\n",
    "        Default: drop na from precursor_intensity column\n",
    "        '''\n",
    "        def drop_na(df, column='precursor_intensity'):\n",
    "            df = df[df[column].notna()]\n",
    "            print(f\"Step 2/12 complete. Dropped rows with NaN for intensities.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Keep only desired charge entires\n",
    "        Default: keep charges 1-6\n",
    "        '''\n",
    "        def keep_desired_charges(df, charge_list=[1, 2, 3, 4, 5, 6]):\n",
    "            df = df[df['precursor_charge'].isin(charge_list)]\n",
    "            print(f\"Step 3/12 complete. Removed charge states not in {charge_list}.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Find all UNIMOD annotations and add them to the vocabulary\n",
    "        (The length of the vocabulary +1 is used later for the embedding layer)\n",
    "        '''\n",
    "        def complete_vocabulary(df):\n",
    "            \"\"\"\n",
    "            Completes the vocabulary with all the possible amino acids and modifications\n",
    "            :return: list\n",
    "            \"\"\"\n",
    "            vocabulary = []\n",
    "            vocabulary+=list('XACDEFGHIKLMNPQRSTVWY')\n",
    "            annotations = re.findall(r'(\\w\\[UNIMOD:\\d+\\])', ' '.join(df['modified_sequence']))\n",
    "            for item in annotations:\n",
    "                if item not in vocabulary:\n",
    "                        vocabulary.append(item)\n",
    "            vocab_len = len(vocabulary)\n",
    "            print(f\"Step 6/12 complete. Completed vocabulary with {vocab_len} entries.\")\n",
    "            return vocabulary, vocab_len\n",
    "            \n",
    "        '''\n",
    "        Combine unique sequences and aggregate their precursor_charges and intensity in order to later select the most abundant charge state per sequence.\n",
    "        '''\n",
    "        def aggregate_sequences(df):\n",
    "            df = df.groupby(\"modified_sequence\")[[\"precursor_charge\", \"precursor_intensity\"]].agg(list).reset_index()\n",
    "            print(f\"Step 4/12 complete. Aggregated all sequences to unique sequences.\")\n",
    "            return df\n",
    "        \n",
    "        # TODO: description\n",
    "        '''\n",
    "        Normalize precursor intensities for aggregated sequences\n",
    "        '''\n",
    "        def normalize_precursor_intensities(df_charge_list, df_intensity_list):\n",
    "            # Get the index of the most abundant precursor intensity\n",
    "            charge_dict = dict()\n",
    "            for index, i in enumerate(df_charge_list):\n",
    "                charge_dict[i] = []\n",
    "                charge_dict[i].append(df_intensity_list[index])\n",
    "        \n",
    "            # Normalize the precursor intensity based on the most abundant precursor intensity\n",
    "            for key, value in charge_dict.items():\n",
    "                if len(value) > 1:\n",
    "                    charge_dict[key] = sum(value) - min(value) / (max(value) - min(value))\n",
    "        \n",
    "            # convert list of one float to float values\n",
    "            charge_dict = {key: value[0] for key, value in charge_dict.items()}\n",
    "            return charge_dict\n",
    "        \n",
    "        # TODO: description\n",
    "        '''\n",
    "        Select most abundand charge state per unique sequence according to the normalized precursor intensity\n",
    "        '''\n",
    "        def get_most_abundant(df_charge_list, df_intensity_list, distributions=False):\n",
    "            charge_dict = dict()\n",
    "            for index, i in enumerate(df_charge_list):\n",
    "                if i not in charge_dict:\n",
    "                    charge_dict[i] = df_intensity_list[index]\n",
    "                else:\n",
    "                    charge_dict[i] += df_intensity_list[index]\n",
    "            if distributions:\n",
    "                return charge_dict\n",
    "            else:\n",
    "                return max(charge_dict, key=charge_dict.get)\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        One-Hot encode most abundand charge state\n",
    "        input: df with \"most_abundance_charge\" column\n",
    "        output: new column \"most_abundant_charge_vector\" containing one-hot encoded vector\n",
    "        '''\n",
    "        def one_hot_encode_charge(df, charge_list=[1, 2, 3, 4, 5, 6]):\n",
    "            df['most_abundant_charge_vector'] = df['most_abundant_charge'].apply(lambda x: [1 if x == i else 0 for i in charge_list])\n",
    "            return df\n",
    "        \n",
    "        # TODO: description\n",
    "        '''\n",
    "        Applying normalization, selecting most abundant charge state and one-hot encoding\n",
    "        '''\n",
    "        def normalize_and_select_most_abundant(df):\n",
    "            df['normalized'] = df.apply(lambda x: normalize_precursor_intensities(x[\"precursor_charge\"], x[\"precursor_intensity\"]), axis=1)\n",
    "            df['pre_normalization'] = df.apply(lambda x: get_most_abundant(x[\"precursor_charge\"], x[\"precursor_intensity\"], True), axis=1)\n",
    "            df['most_abundant_charge'] = df['normalized'].apply(lambda x: max(x, key=x.get))\n",
    "            df = one_hot_encode_charge(df)\n",
    "            print(f\"Step 8/12 complete. Applied normalization, selected most abundant charge state and one-hot encoded it.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        get topK charge states for each sequence according to the normalized precursor intensity\n",
    "        \n",
    "        input: df with \"normalized\" column\n",
    "        output: new column \"topK_charge_states\" containing list of topK charge states\n",
    "        \n",
    "        default: k=2\n",
    "        '''\n",
    "        def get_topK_charge_states(df, k=2):\n",
    "            def get_topK(label_dict):\n",
    "                allowed_keys = list()\n",
    "                sorted_values = sorted(label_dict.values(), reverse=True)\n",
    "                for i in sorted_values:\n",
    "                    for key, value in label_dict.items():\n",
    "                        if i == value and len(allowed_keys) <= k-1:\n",
    "                            allowed_keys.append(key)\n",
    "                return allowed_keys\n",
    "        \n",
    "            df[f'top_{k}_charge_states'] = df['normalized'].apply(get_topK)\n",
    "            print(f\"Step 11/12 complete. Selected top {k} charge states per sequence.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Remove sequences of specific length represented less than a certain number of times\n",
    "        \n",
    "        input: df containig \"modified_sequence\" column, representation_threshold\n",
    "        output: \n",
    "        - df containing only sequence legths represented more than representation_threshold times\n",
    "        - padding_length\n",
    "        default: representation_threshold = 100\n",
    "        \n",
    "        Calculate the sequence lengths and their counts\n",
    "        Filter out sequences with counts below the threshold\n",
    "        Filter the original DataFrame based on sequence length\n",
    "        Drop the temporary column\n",
    "        '''\n",
    "        def remove_rare_sequence_lengths(df, representation_threshold=100):\n",
    "            before_len = len(df)\n",
    "            df['sequence_length_prepadding'] = df['modified_sequence'].apply(len)\n",
    "            len_counts = df['sequence_length_prepadding'].value_counts().reset_index()\n",
    "            len_counts.columns = ['seq_len', 'count']\n",
    "            filtered_lengths = len_counts[len_counts['count'] >= representation_threshold]['seq_len']\n",
    "            df = df[df['sequence_length_prepadding'].isin(filtered_lengths)].copy()\n",
    "            padding_length = df['sequence_length_prepadding'].max()\n",
    "            df = df[df['sequence_length_prepadding'].isin(filtered_lengths)]\n",
    "            after_len = len(df)\n",
    "            print(f\"Step 5/12 complete. Removed {before_len - after_len} of {before_len} sequences if sequence-length is represented less than {representation_threshold} times.\")\n",
    "            return df, padding_length\n",
    "        \n",
    "        '''\n",
    "        Encode all occuring charge states per unique sequence in a binary vector\n",
    "        \n",
    "        input: df containing \"precursor_charge\" column\n",
    "        output: df containing an additional \"charge_state_vector\" column encoding all occuring charge states per unique sequence in a binary vector\n",
    "        '''\n",
    "        def encode_charge_states(df):\n",
    "            df['charge_state_vector'] = df['precursor_charge'].apply(lambda x: [1 if i in x else 0 for i in range(1,7)])\n",
    "            print(f\"Step 9/12 complete. Encoded all occuring charge states per unique sequence in a binary vector.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Checks if a vector contains only continous charge states e.g. [1,1,1,0,0,0]\n",
    "        Flase if a vector contains skipped charges e.g. [1,0,0,0,0,1]\n",
    "        \n",
    "        input: charge_state_vector\n",
    "        output: True if no charge state is skipped, False if a charge state is skipped\n",
    "        '''\n",
    "        def has_skipped_charges(charge_state_vector):\n",
    "            was_found = False\n",
    "            was_concluded = False\n",
    "            for i in charge_state_vector:\n",
    "                if i == 1 and not was_found:\n",
    "                    was_found = True\n",
    "                if i == 0 and was_found:\n",
    "                    was_concluded = True\n",
    "                if i == 1 and was_concluded:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        '''\n",
    "        Filter out all sequences where has_skipped_charges() returns True\n",
    "        \n",
    "        input: df containing \"charge_state_vector\" column\n",
    "        output: df containing only sequences where has_skipped_charges() returns False\n",
    "        '''\n",
    "        def filter_skipped_charges(df):\n",
    "            return df[df['charge_state_vector'].apply(lambda x: not has_skipped_charges(x))]\n",
    "        \n",
    "        '''\n",
    "        Removes sequences with skipped charges that occur less than a certain number of times\n",
    "        \n",
    "        input: df containing \"charge_state_vector\" column, cutoff\n",
    "        output: df containing only sequences with skipped charges that occur more than cutoff times\n",
    "        default: cutoff = 1000\n",
    "        '''\n",
    "        def skip_charges_for_occurrences(df, cutoff = 1000):\n",
    "            list_k = []\n",
    "            list_v = []\n",
    "            drop_out_index = []\n",
    "            for index, i in enumerate(df['charge_state_vector'].value_counts()):\n",
    "                list_k.append(df['charge_state_vector'].value_counts().index[index])\n",
    "                list_v.append(i)\n",
    "                if  has_skipped_charges(df['charge_state_vector'].value_counts().index[index]) and list_v[index] < cutoff:\n",
    "                    drop_out_index.append(index)\n",
    "                    \n",
    "            drop_out_list = []\n",
    "            for i in drop_out_index:\n",
    "                drop_out_list.append(list_k[i])\n",
    "            df_out = df[~df['charge_state_vector'].isin(drop_out_list)]\n",
    "            print(f\"Step 10/12 complete. Removed {len(df) - len(df_out)} of {len(df)} sequences if unique charge state distribution is represented less than {cutoff} times.\")\n",
    "            return df_out    \n",
    "                    \n",
    "        \"\"\"\n",
    "        Encodes the 'modified_sequence' column in a DataFrame and adds a new column 'modified_sequence_vector'.\n",
    "        \n",
    "        input: df containing \"modified_sequence\" column, vocabulary, padding_length\n",
    "        output: df containing \"modified_sequence_vector\" column with padded and encoded sequences\n",
    "        \n",
    "        defaults: padding_length = 50\n",
    "        \"\"\"\n",
    "        def sequence_encoder(df, padding_length=50, vocabulary=None):\n",
    "            \n",
    "            if 'modified_sequence' not in df.columns:\n",
    "                raise ValueError(\"DataFrame must contain a 'modified_sequence' column.\")\n",
    "        \n",
    "            aa_dictionary = {aa: index for index, aa in enumerate(vocabulary)}\n",
    "        \n",
    "            def encode_sequence(sequence):\n",
    "                pattern = r'[A-Z]\\[[^\\]]*\\]|.'\n",
    "                result = [match for match in re.findall(pattern, sequence)]\n",
    "                result += ['X'] * (padding_length - len(result))\n",
    "                return [aa_dictionary.get(aa, aa_dictionary['X']) for aa in result]\n",
    "        \n",
    "            df['modified_sequence_vector'] = df['modified_sequence'].apply(encode_sequence)\n",
    "            print(f\"Step 7/12 complete. Encoded all sequences.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Generate overview plot for precursor_charge distribution in combined dataset\n",
    "        '''\n",
    "        def plot_most_abundant_charge_distribution(df):\n",
    "            # plot the distirbution of precursor_charge for the whole dataset\n",
    "            sns.set_theme(style=\"darkgrid\")\n",
    "            sns.set_context(\"paper\")\n",
    "            ax = sns.countplot(x='most_abundant_charge', data=df, palette=\"viridis\")\n",
    "            plt.xlabel('Precursor Charge')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Distribution of Precursor Charge')\n",
    "            # add percentage of each charge state to the plot\n",
    "            total = len(df['most_abundant_charge'])\n",
    "            for p in ax.patches:\n",
    "                percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "                x = p.get_x() + p.get_width() / 2 - 0.05\n",
    "                y = p.get_y() + p.get_height() + 5\n",
    "                ax.annotate(percentage, (x, y))\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_topK_charge_distribution(df, column_name='top_2_charge_states'):\n",
    "            charge_state_counter = {\n",
    "                1: 0,\n",
    "                2: 0,\n",
    "                3: 0,\n",
    "                4: 0,\n",
    "                5: 0,\n",
    "                6: 0\n",
    "            }\n",
    "            \n",
    "            for row in df[column_name]:\n",
    "                for k in row:\n",
    "                    charge_state_counter[k] = charge_state_counter[k] + 1\n",
    "            sns.set_theme(style=\"darkgrid\")\n",
    "            sns.set_context(\"paper\")\n",
    "            palette = sns.color_palette(\"viridis\", len(charge_state_counter))\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.bar(range(len(charge_state_counter)), list(charge_state_counter.values()), align='center', color=palette)\n",
    "            plt.xticks(range(len(charge_state_counter)), list(charge_state_counter.keys()))\n",
    "            plt.xlabel('Charge State')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Charge State Distribution')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            total = len(df['most_abundant_charge'])\n",
    "            for p in plt.gca().patches:\n",
    "                height = p.get_height()\n",
    "                plt.gca().text(p.get_x() + p.get_width()/2., height + 3, '{:.1f}%'.format(height/total*100), ha='center', fontsize=12)\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        self.dir_path = dir_path\n",
    "        self.file_type = file_type\n",
    "        \n",
    "        self.charge_states = charge_states\n",
    "        self.num_classes = len(self.charge_states)\n",
    "        \n",
    "        self.data_types = ['array', 'tensor', '2d_tensor']\n",
    "        self.data_type = data_type\n",
    "        \n",
    "        self.classification_types = ['multi_class', 'multi_label']\n",
    "        self.classification_type = classification_type\n",
    "        \n",
    "        self.model_types = ['embedding', 'conv2d', 'prosit']\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        self.df = combine_parquet_into_df(dir_path, file_type)\n",
    "        self.df = drop_na(self.df, 'precursor_intensity')\n",
    "        self.df = keep_desired_charges(self.df)\n",
    "        self.df = aggregate_sequences(self.df)\n",
    "        self.df, self.padding_length = remove_rare_sequence_lengths(self.df)\n",
    "        self.vocabulary, self.voc_len = complete_vocabulary(self.df)\n",
    "        self.df = sequence_encoder(self.df, self.padding_length, self.vocabulary)\n",
    "        self.df = normalize_and_select_most_abundant(self.df)\n",
    "        self.df = encode_charge_states(self.df)\n",
    "        self.df = skip_charges_for_occurrences(self.df)\n",
    "        self.df = get_topK_charge_states(self.df)\n",
    "        if self.classification_type == 'multi_class':\n",
    "            self.df = self.df[['modified_sequence_vector', 'most_abundant_charge_vector', 'top_2_charge_states']]\n",
    "        elif self.classification_type == 'multi_label':\n",
    "            self.df = self.df[['modified_sequence_vector', 'charge_state_vector', 'top_2_charge_states']]\n",
    "        else:\n",
    "            raise ValueError(\"classification_type must be one of the following: 'multi_class', 'multi_label'\")\n",
    "        \n",
    "        self.validation_ratio = 0.2\n",
    "        self.test_mode = True\n",
    "        self.test_ratio = 0.1\n",
    "        self.df_test = self.df.sample(frac = self.test_ratio)\n",
    "        self.training_validation_df = df.drop(self.df_test.index)\n",
    "        self.training_validation_split = StratifiedShuffleSplit(n_splits=1, test_size=self.validation_ratio)\n",
    "\n",
    "        \n",
    "        def create_training_validation_split(df = self.training_validation_df, sssplit = self.training_validation_split):\n",
    "            trainval_ds_embed = np.array(df['modified_sequence_vector']) # TODO\n",
    "            trainval_labels_embed = np.array(df['most_abundant_charge_vector'])\n",
    "            # Perform the split train and val\n",
    "            train_indicies_embed, val_indicies_embed = next(sssplit.split(trainval_ds_embed, trainval_labels_embed))\n",
    "            # Distribution\n",
    "            train_ds_embed, train_labels_embed = trainval_ds_embed[train_indicies_embed], trainval_labels_embed[train_indicies_embed]\n",
    "            val_ds_embed, val_labels_embed = trainval_ds_embed[val_indicies_embed], trainval_labels_embed[val_indicies_embed]\n",
    "            # create two dataframes for training and validation\n",
    "            df_train = pd.DataFrame({'modified_sequence_vector': train_ds_embed, 'most_abundant_charge_vector': train_labels_embed})\n",
    "            df_val = pd.DataFrame({'modified_sequence_vector': val_ds_embed, 'most_abundant_charge_vector': val_labels_embed})\n",
    "            return df_train, df_val\n",
    "        self.df_train, self.df_val = create_training_validation_split(self.training_validation_df, self.training_validation_split)\n",
    "                    \n",
    "        def to_array(df):   \n",
    "            label = [np.array(x) for x in df['most_abundant_charge_vector']]\n",
    "            data = [np.array(x) for x in df['modified_sequence_vector']]\n",
    "            return label, data\n",
    "        def to_tensor(df):\n",
    "            label, data = to_array(df)\n",
    "            label = tf.convert_to_tensor(label)\n",
    "            data = tf.convert_to_tensor(data)\n",
    "            return label, data\n",
    "        def to_2d_tensor(df):\n",
    "            label, data = to_array(df)\n",
    "            label = tf.convert_to_tensor(label)\n",
    "            data = [np.reshape(np.array(x), (1, self.padding_length, 1)) for x in data]\n",
    "            return label, data\n",
    "        \n",
    "        if self.data_type == \"array\":\n",
    "            self.test_label, self.test_data = to_array(self.df_test)\n",
    "            self.train_label, self.train_data = to_array(self.df_train)\n",
    "            self.data_type, self.val_data = to_array(self.df_val)\n",
    "        elif self.type == \"tensor\":\n",
    "            self.test_label, self.test_data = to_tensor(self.df_test)\n",
    "            self.train_label, self.train_data = to_tensor(self.df_train)\n",
    "            self.val_label, self.val_data = to_tensor(self.df_val)\n",
    "        elif self.data_type == \"2d_tensor\":\n",
    "            self.test_label, self.test_data = to_2d_tensor(self.df_test)\n",
    "            self.train_label, self.train_data = to_2d_tensor(self.df_train)\n",
    "            self.val_label, self.val_data = to_2d_tensor(self.df_val)\n",
    "\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/12 complete. Combined 12 files into one DataFrame.\n",
      "Step 2/12 complete. Dropped rows with NaN for intensities.\n",
      "Step 3/12 complete. Removed charge states not in [1, 2, 3, 4, 5, 6].\n",
      "Step 4/12 complete. Aggregated all sequences to unique sequences.\n",
      "Step 5/12 complete. Removed 857 of 831677 sequences if sequence-length is represented less than 100 times.\n",
      "Step 6/12 complete. Completed vocabulary with 23 entries.\n",
      "Step 7/12 complete. Encoded all sequences.\n",
      "Step 8/12 complete. Applied normalization, selected most abundant charge state and one-hot encoded it.\n",
      "Step 9/12 complete. Encoded all occuring charge states per unique sequence in a binary vector.\n",
      "Step 10/12 complete. Removed 728 of 830820 sequences if unique charge state distribution is represented less than 1000 times.\n",
      "Step 11/12 complete. Selected top 2 charge states per sequence.\n"
     ]
    }
   ],
   "source": [
    "my_dataset = PrecursorChargeStateDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:48:27.111254700Z",
     "start_time": "2023-09-12T12:46:18.743674700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_dataset.test_label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.997641400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Models\n",
    "\n",
    "General idea:\n",
    "Input: mod_seq_encoded, precursor_charge // precursor_charge_onehot\n",
    "Output: 5 nodes --> highest value == most probable charge for input sequence\n",
    "Use: Softmax, Crossentropy loss\n",
    "\n",
    "stratified split:\n",
    "- PROSITE\n",
    "- CCE\n",
    "- SCCE // ?\n",
    "\n",
    "evaluate models by:\n",
    "- categorical accuracy\n",
    "- f1 score // ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Stratified split for all models\n",
    "Split into train_val and test by unique sequence \n",
    "\n",
    "Split into train and val via stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:39.999653800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### CCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:40.000655800Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import Sequential, Model\n",
    "from keras.layers import Embedding, Flatten, Dense, Input\n",
    "\n",
    "inputA_embed = Input(shape=train_ds_embed[0].shape)\n",
    "x = Model(inputs=inputA_embed, outputs=inputA_embed)\n",
    "y = Embedding(input_dim=voc_len, output_dim=max_len_seq, input_length=max_len_seq)(inputA_embed)\n",
    "y = Flatten()(y)\n",
    "y = Dense(max_len_seq, activation=\"relu\")(y)\n",
    "z = Dense(num_classes, activation=\"softmax\")(y)\n",
    "multiclass_model = Model(inputs=[x.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:40.001651600Z"
    }
   },
   "outputs": [],
   "source": [
    "multiclass_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:40.002650100Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(multiclass_model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:40.003649600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define callbacks for multiclass model\n",
    "checkpoint_callback_multiclass_model = ModelCheckpoint('checkpoints/multiclass_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping_multiclass_model = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Compile the model\n",
    "multiclass_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:40.004650300Z"
    }
   },
   "outputs": [],
   "source": [
    "# training with stratified split\n",
    "# save history of model training\n",
    "history_multiclass_model = multiclass_model.fit(train_ds_embed, train_labels_embed, epochs=30, batch_size=4096, validation_data=(val_ds_embed, val_labels_embed), callbacks=[checkpoint_callback_embed, early_stopping_embed, WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anaylsis / Plots etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:40.005651900Z"
    }
   },
   "outputs": [],
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:40.005651900Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_charge_prediction_text(charge_predictions):\n",
    "    max_charge_index = np.argmax(charge_predictions)\n",
    "    max_charge_value = round(charge_predictions[max_charge_index], 2)\n",
    "\n",
    "    charge_text = f\"The predicted charge state for the input sequence is {max_charge_index+1} [{round(max_charge_value*100,2)}%].\"\n",
    "    percentage_text = \"Prediction percentages for other states:\\n\"\n",
    "\n",
    "    for index, prediction in enumerate(charge_predictions):\n",
    "        if index != max_charge_index:\n",
    "            percentage = round(prediction * 100, 2)\n",
    "            percentage_text += f\"Charge state {index+1}: {percentage}%\\n\"\n",
    "\n",
    "    full_text = charge_text + \"\\n\" + percentage_text\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# Beispiel\n",
    "charge_predictions = np.array([0, 0.3, 0.53, 0.17, 0, 0])\n",
    "output_text = generate_charge_prediction_text(charge_predictions)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-12T12:02:40.006650100Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
