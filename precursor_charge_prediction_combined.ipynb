{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data preprocessing\n",
    "- remove all columns except: 'modified_sequence', 'precursor_charge', 'precursor_intensity'\n",
    "- filter out unwanted charge states\n",
    "- filter for most abundant charge state per sequence by precursor_intensity after normalizing the values\n",
    "- filter sequence length according to occurance in dataset (currently less than 100 sequences of a certain length get removed)\n",
    "- search for occurences of UNIMOD modifications and add them to the vocabulary\n",
    "- generate continous sequence encoding // first layer - embedding layer\n",
    "- generate precursor_charge one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dataset-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T10:54:20.124024600Z",
     "start_time": "2023-09-13T10:54:15.312238900Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import timeit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T10:54:20.124024600Z",
     "start_time": "2023-09-13T10:54:20.115430200Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "File import for .parquet, .tsv and .csv files\n",
    "At the moment a mix of .parquet, .tsv and .csv files will also be combined into one dataframe.\n",
    "'''\n",
    "def combine_files_into_df(dir_path='data/', file_types=['.parquet', '.tsv', '.csv'], columns_to_keep=['modified_sequence', 'precursor_charge', 'precursor_intensity']):\n",
    "    dfs = []\n",
    "    \n",
    "    for file in os.listdir(dir_path):\n",
    "        if any(file.endswith(file_type) for file_type in file_types):\n",
    "            file_path = os.path.join(dir_path, file)\n",
    "            \n",
    "            if file.endswith('.parquet'):\n",
    "                df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "            elif file.endswith('.tsv'):\n",
    "                df = pd.read_csv(file_path, sep='\\t')\n",
    "            elif file.endswith('.csv'):\n",
    "                df = pd.read_csv(file_path)\n",
    "            else:\n",
    "                continue  # Skip unsupported file types\n",
    "            \n",
    "            df = df[columns_to_keep]\n",
    "            dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:16:27.296887500Z",
     "start_time": "2023-09-13T14:16:26.996489500Z"
    }
   },
   "outputs": [],
   "source": [
    "class PrecursorChargeStateDataset:\n",
    "    def __init__(self, classification_type=\"multi_class\", model_type=\"embedding\", charge_states=[1, 2, 3, 4, 5, 6], dir_path='data/', file_type='.parquet', columns_to_keep=['modified_sequence','precursor_charge','precursor_intensity'], test_ratio=0.1):\n",
    "        \n",
    "        \n",
    "        ''' CHECK ALL INPUTS '''\n",
    "       \n",
    "        # check if classification_type is valid\n",
    "        if isinstance(classification_type, str):\n",
    "            if classification_type not in [\"multi_class\", \"multi_label\"]:\n",
    "                raise ValueError(\"classification_type must be either 'multi_class' or 'multi_label'.\")\n",
    "            else:\n",
    "                classification_type = classification_type.lower()\n",
    "        else:\n",
    "            raise TypeError(\"classification_type must be a string.\")\n",
    "        \n",
    "        # check if model_type is valid\n",
    "        if isinstance(model_type, str):\n",
    "            if model_type not in [\"embedding\", \"conv2d\", \"prosit\"]:\n",
    "                raise ValueError(\"model_type must be 'embedding', 'conv2d', 'prosit'.\")\n",
    "            else:\n",
    "                model_type = model_type.lower()\n",
    "        else:\n",
    "            raise TypeError(\"model_type must be a string.\")\n",
    "        \n",
    "        # check if classification_type and model_type are compatible\n",
    "        if classification_type == \"multi_class\" and not model_type in [\"embedding\", \"conv2d\", \"prosit\"]:\n",
    "            raise ValueError(\"classification_type and model_type are not compatible.\")\n",
    "        elif classification_type in [\"multi_label\", \"multi_head\"] and not model_type in [\"embedding\"]:\n",
    "            raise ValueError(\"classification_type and model_type are not compatible.\")\n",
    "            \n",
    "        \n",
    "        \n",
    "        # check if charge states correct:\n",
    "        if isinstance(charge_states, list):\n",
    "            if not all(isinstance(item, int) for item in charge_states):\n",
    "                raise ValueError(\"charge_states must be a list of integers.\")\n",
    "        else:\n",
    "            raise TypeError(\"charge_states must be a list.\")\n",
    "        \n",
    "        # check dir_path\n",
    "        if isinstance(dir_path, str):\n",
    "            if not os.path.isdir(dir_path):\n",
    "                raise ValueError(\"dir_path must be a valid directory. Is not: {}\".format(dir_path))\n",
    "        else:\n",
    "            raise TypeError(\"dir_path must be a string.\")\n",
    "        \n",
    "        # check file_type\n",
    "        if isinstance(file_type, str):\n",
    "            if not file_type.startswith(\".\"):\n",
    "                file_type = \".\" + file_type\n",
    "        else:\n",
    "            raise TypeError(\"file_type must be a string.\")\n",
    "        \n",
    "        # check columns_to_keep\n",
    "        if isinstance(columns_to_keep, list):\n",
    "            if not all(isinstance(item, str) for item in columns_to_keep):\n",
    "                raise ValueError(\"columns_to_keep must be a list of strings. In Order: 'modified_sequence', 'precursor_charge', 'precursor_intensity'.\")\n",
    "        else:\n",
    "            raise TypeError(\"columns_to_keep must be a list.\")\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Combine files into one dataframe and keep only desired columns\n",
    "        Default: \n",
    "        - dir_path = 'data/'\n",
    "        - file_type = '.parquet'\n",
    "        Default: drop everything except: modified_sequence, precursor_charge and precursor_intensity\n",
    "        - columns_to_keep = ['modified_sequence','precursor_charge','precursor_intensity']\n",
    "        '''\n",
    "        def combine_parquet_into_df(dir_path='data/', file_type='.parquet', columns_to_keep=['modified_sequence','precursor_charge','precursor_intensity']):\n",
    "            dfs = [] \n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith(file_type):\n",
    "                    file_path = os.path.join(dir_path, file)\n",
    "                    df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "                    df = df[columns_to_keep]\n",
    "                    dfs.append(df)\n",
    "        \n",
    "            df = pd.concat(dfs, ignore_index=True)\n",
    "            print(f\"Step 1/12 complete. Combined {len(dfs)} files into one DataFrame.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Drop all rows with NaN values in a specific column\n",
    "        Default: drop na from precursor_intensity column\n",
    "        '''\n",
    "        def drop_na(df, column='precursor_intensity'):\n",
    "            df = df[df[column].notna()]\n",
    "            print(f\"Step 2/12 complete. Dropped rows with NaN for intensities.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Keep only desired charge entires\n",
    "        Default: keep charges 1-6\n",
    "        '''\n",
    "        def keep_desired_charges(df, charge_list=[1, 2, 3, 4, 5, 6]):\n",
    "            df = df[df['precursor_charge'].isin(charge_list)]\n",
    "            print(f\"Step 3/12 complete. Removed charge states not in {charge_list}.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Find all UNIMOD annotations and add them to the vocabulary\n",
    "        (The length of the vocabulary +1 is used later for the embedding layer)\n",
    "        '''\n",
    "        def complete_vocabulary(df):\n",
    "            \"\"\"\n",
    "            Completes the vocabulary with all the possible amino acids and modifications\n",
    "            :return: list\n",
    "            \"\"\"\n",
    "            vocabulary = []\n",
    "            vocabulary+=list('XACDEFGHIKLMNPQRSTVWY')\n",
    "            annotations = re.findall(r'(\\w\\[UNIMOD:\\d+\\])', ' '.join(df['modified_sequence']))\n",
    "            for item in annotations:\n",
    "                if item not in vocabulary:\n",
    "                        vocabulary.append(item)\n",
    "            vocab_len = len(vocabulary)\n",
    "            print(f\"Step 6/12 complete. Completed vocabulary with {vocab_len} entries.\")\n",
    "            return vocabulary, vocab_len\n",
    "            \n",
    "        '''\n",
    "        Combine unique sequences and aggregate their precursor_charges and intensity in order to later select the most abundant charge state per sequence.\n",
    "        '''\n",
    "        def aggregate_sequences(df):\n",
    "            df = df.groupby(\"modified_sequence\")[[\"precursor_charge\", \"precursor_intensity\"]].agg(list).reset_index()\n",
    "            print(f\"Step 4/12 complete. Aggregated all sequences to unique sequences.\")\n",
    "            return df\n",
    "        \n",
    "        # TODO: description\n",
    "        '''\n",
    "        Normalize precursor intensities for aggregated sequences\n",
    "        '''\n",
    "        def normalize_precursor_intensities(df_charge_list, df_intensity_list):\n",
    "            # Get the index of the most abundant precursor intensity\n",
    "            charge_dict = dict()\n",
    "            for index, i in enumerate(df_charge_list):\n",
    "                charge_dict[i] = []\n",
    "                charge_dict[i].append(df_intensity_list[index])\n",
    "        \n",
    "            # Normalize the precursor intensity based on the most abundant precursor intensity\n",
    "            for key, value in charge_dict.items():\n",
    "                if len(value) > 1:\n",
    "                    charge_dict[key] = sum(value) - min(value) / (max(value) - min(value))\n",
    "        \n",
    "            # convert list of one float to float values\n",
    "            charge_dict = {key: value[0] for key, value in charge_dict.items()}\n",
    "            return charge_dict\n",
    "        \n",
    "        # TODO: description\n",
    "        '''\n",
    "        Select most abundand charge state per unique sequence according to the normalized precursor intensity\n",
    "        '''\n",
    "        def get_most_abundant(df_charge_list, df_intensity_list, distributions=False):\n",
    "            charge_dict = dict()\n",
    "            for index, i in enumerate(df_charge_list):\n",
    "                if i not in charge_dict:\n",
    "                    charge_dict[i] = df_intensity_list[index]\n",
    "                else:\n",
    "                    charge_dict[i] += df_intensity_list[index]\n",
    "            if distributions:\n",
    "                return charge_dict\n",
    "            else:\n",
    "                return max(charge_dict, key=charge_dict.get)\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        One-Hot encode most abundand charge state\n",
    "        input: df with \"most_abundance_charge\" column\n",
    "        output: new column \"most_abundant_charge_vector\" containing one-hot encoded vector\n",
    "        '''\n",
    "        def one_hot_encode_charge(df, charge_list=[1, 2, 3, 4, 5, 6]):\n",
    "            df['most_abundant_charge_vector'] = df['most_abundant_charge'].apply(lambda x: [1 if x == i else 0 for i in charge_list])\n",
    "            return df\n",
    "        \n",
    "        # TODO: description\n",
    "        '''\n",
    "        Applying normalization, selecting most abundant charge state and one-hot encoding\n",
    "        '''\n",
    "        def normalize_and_select_most_abundant(df):\n",
    "            df['normalized'] = df.apply(lambda x: normalize_precursor_intensities(x[\"precursor_charge\"], x[\"precursor_intensity\"]), axis=1)\n",
    "            df['pre_normalization'] = df.apply(lambda x: get_most_abundant(x[\"precursor_charge\"], x[\"precursor_intensity\"], True), axis=1)\n",
    "            df['most_abundant_charge'] = df['normalized'].apply(lambda x: max(x, key=x.get))\n",
    "            df = one_hot_encode_charge(df)\n",
    "            print(f\"Step 8/12 complete. Applied normalization, selected most abundant charge state and one-hot encoded it.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        get topK charge states for each sequence according to the normalized precursor intensity\n",
    "        \n",
    "        input: df with \"normalized\" column\n",
    "        output: new column \"topK_charge_states\" containing list of topK charge states\n",
    "        \n",
    "        default: k=2\n",
    "        '''\n",
    "        def get_topK_charge_states(df, k=2):\n",
    "            def get_topK(label_dict):\n",
    "                allowed_keys = list()\n",
    "                sorted_values = sorted(label_dict.values(), reverse=True)\n",
    "                for i in sorted_values:\n",
    "                    for key, value in label_dict.items():\n",
    "                        if i == value and len(allowed_keys) <= k-1:\n",
    "                            allowed_keys.append(key)\n",
    "                return allowed_keys\n",
    "        \n",
    "            df[f'top_{k}_charge_states'] = df['normalized'].apply(get_topK)\n",
    "            print(f\"Step 11/12 complete. Selected top {k} charge states per sequence.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Remove sequences of specific length represented less than a certain number of times\n",
    "        \n",
    "        input: df containig \"modified_sequence\" column, representation_threshold\n",
    "        output: \n",
    "        - df containing only sequence legths represented more than representation_threshold times\n",
    "        - padding_length\n",
    "        default: representation_threshold = 100\n",
    "        \n",
    "        Calculate the sequence lengths and their counts\n",
    "        Filter out sequences with counts below the threshold\n",
    "        Filter the original DataFrame based on sequence length\n",
    "        Drop the temporary column\n",
    "        '''\n",
    "        def remove_rare_sequence_lengths(df, representation_threshold=100):\n",
    "            before_len = len(df)\n",
    "            df['sequence_length_prepadding'] = df['modified_sequence'].apply(len)\n",
    "            len_counts = df['sequence_length_prepadding'].value_counts().reset_index()\n",
    "            len_counts.columns = ['seq_len', 'count']\n",
    "            filtered_lengths = len_counts[len_counts['count'] >= representation_threshold]['seq_len']\n",
    "            df = df[df['sequence_length_prepadding'].isin(filtered_lengths)].copy()\n",
    "            padding_length = df['sequence_length_prepadding'].max()\n",
    "            df = df[df['sequence_length_prepadding'].isin(filtered_lengths)]\n",
    "            after_len = len(df)\n",
    "            print(f\"Step 5/12 complete. Removed {before_len - after_len} of {before_len} sequences if sequence-length is represented less than {representation_threshold} times.\")\n",
    "            return df, padding_length\n",
    "        \n",
    "        '''\n",
    "        Encode all occuring charge states per unique sequence in a binary vector\n",
    "        \n",
    "        input: df containing \"precursor_charge\" column\n",
    "        output: df containing an additional \"charge_state_vector\" column encoding all occuring charge states per unique sequence in a binary vector\n",
    "        '''\n",
    "        def encode_charge_states(df):\n",
    "            df['charge_state_vector'] = df['precursor_charge'].apply(lambda x: [1 if i in x else 0 for i in range(1,7)])\n",
    "            print(f\"Step 9/12 complete. Encoded all occuring charge states per unique sequence in a binary vector.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Checks if a vector contains only continous charge states e.g. [1,1,1,0,0,0]\n",
    "        Flase if a vector contains skipped charges e.g. [1,0,0,0,0,1]\n",
    "        \n",
    "        input: charge_state_vector\n",
    "        output: True if no charge state is skipped, False if a charge state is skipped\n",
    "        '''\n",
    "        def has_skipped_charges(charge_state_vector):\n",
    "            was_found = False\n",
    "            was_concluded = False\n",
    "            for i in charge_state_vector:\n",
    "                if i == 1 and not was_found:\n",
    "                    was_found = True\n",
    "                if i == 0 and was_found:\n",
    "                    was_concluded = True\n",
    "                if i == 1 and was_concluded:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        '''\n",
    "        Filter out all sequences where has_skipped_charges() returns True\n",
    "        \n",
    "        input: df containing \"charge_state_vector\" column\n",
    "        output: df containing only sequences where has_skipped_charges() returns False\n",
    "        '''\n",
    "        def filter_skipped_charges(df):\n",
    "            return df[df['charge_state_vector'].apply(lambda x: not has_skipped_charges(x))]\n",
    "        \n",
    "        '''\n",
    "        Removes sequences with skipped charges that occur less than a certain number of times\n",
    "        \n",
    "        input: df containing \"charge_state_vector\" column, cutoff\n",
    "        output: df containing only sequences with skipped charges that occur more than cutoff times\n",
    "        default: cutoff = 1000\n",
    "        '''\n",
    "        def skip_charges_for_occurrences(df, cutoff = 1000):\n",
    "            list_k = []\n",
    "            list_v = []\n",
    "            drop_out_index = []\n",
    "            for index, i in enumerate(df['charge_state_vector'].value_counts()):\n",
    "                list_k.append(df['charge_state_vector'].value_counts().index[index])\n",
    "                list_v.append(i)\n",
    "                if  has_skipped_charges(df['charge_state_vector'].value_counts().index[index]) and list_v[index] < cutoff:\n",
    "                    drop_out_index.append(index)\n",
    "                    \n",
    "            drop_out_list = []\n",
    "            for i in drop_out_index:\n",
    "                drop_out_list.append(list_k[i])\n",
    "            df_out = df[~df['charge_state_vector'].isin(drop_out_list)]\n",
    "            print(f\"Step 10/12 complete. Removed {len(df) - len(df_out)} of {len(df)} sequences if unique charge state distribution is represented less than {cutoff} times.\")\n",
    "            return df_out    \n",
    "                    \n",
    "        \"\"\"\n",
    "        Encodes the 'modified_sequence' column in a DataFrame and adds a new column 'modified_sequence_vector'.\n",
    "        \n",
    "        input: df containing \"modified_sequence\" column, vocabulary, padding_length\n",
    "        output: df containing \"modified_sequence_vector\" column with padded and encoded sequences\n",
    "        \n",
    "        defaults: padding_length = 50\n",
    "        \"\"\"\n",
    "        def sequence_encoder(df, padding_length=50, vocabulary=None):\n",
    "            \n",
    "            if 'modified_sequence' not in df.columns:\n",
    "                raise ValueError(\"DataFrame must contain a 'modified_sequence' column.\")\n",
    "        \n",
    "            aa_dictionary = {aa: index for index, aa in enumerate(vocabulary)}\n",
    "        \n",
    "            def encode_sequence(sequence):\n",
    "                pattern = r'[A-Z]\\[[^\\]]*\\]|.'\n",
    "                result = [match for match in re.findall(pattern, sequence)]\n",
    "                result += ['X'] * (padding_length - len(result))\n",
    "                return [aa_dictionary.get(aa, aa_dictionary['X']) for aa in result]\n",
    "        \n",
    "            df['modified_sequence_vector'] = df['modified_sequence'].apply(encode_sequence)\n",
    "            print(f\"Step 7/12 complete. Encoded all sequences.\")\n",
    "            return df\n",
    "        \n",
    "        '''\n",
    "        Generate overview plot for precursor_charge distribution in combined dataset\n",
    "        '''\n",
    "        def plot_most_abundant_charge_distribution(df):\n",
    "            # plot the distirbution of precursor_charge for the whole dataset\n",
    "            sns.set_theme(style=\"darkgrid\")\n",
    "            sns.set_context(\"paper\")\n",
    "            ax = sns.countplot(x='most_abundant_charge', data=df, palette=\"viridis\")\n",
    "            plt.xlabel('Precursor Charge')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Distribution of Precursor Charge')\n",
    "            # add percentage of each charge state to the plot\n",
    "            total = len(df['most_abundant_charge'])\n",
    "            for p in ax.patches:\n",
    "                percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "                x = p.get_x() + p.get_width() / 2 - 0.05\n",
    "                y = p.get_y() + p.get_height() + 5\n",
    "                ax.annotate(percentage, (x, y))\n",
    "            plt.show()\n",
    "            \n",
    "        def plot_topK_charge_distribution(df, column_name='top_2_charge_states'):\n",
    "            charge_state_counter = {\n",
    "                1: 0,\n",
    "                2: 0,\n",
    "                3: 0,\n",
    "                4: 0,\n",
    "                5: 0,\n",
    "                6: 0\n",
    "            }\n",
    "            \n",
    "            for row in df[column_name]:\n",
    "                for k in row:\n",
    "                    charge_state_counter[k] = charge_state_counter[k] + 1\n",
    "            sns.set_theme(style=\"darkgrid\")\n",
    "            sns.set_context(\"paper\")\n",
    "            palette = sns.color_palette(\"viridis\", len(charge_state_counter))\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.bar(range(len(charge_state_counter)), list(charge_state_counter.values()), align='center', color=palette)\n",
    "            plt.xticks(range(len(charge_state_counter)), list(charge_state_counter.keys()))\n",
    "            plt.xlabel('Charge State')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Charge State Distribution')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            total = len(df['most_abundant_charge'])\n",
    "            for p in plt.gca().patches:\n",
    "                height = p.get_height()\n",
    "                plt.gca().text(p.get_x() + p.get_width()/2., height + 3, '{:.1f}%'.format(height/total*100), ha='center', fontsize=12)\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        self.dir_path = dir_path\n",
    "        self.file_type = file_type\n",
    "        \n",
    "        self.charge_states = charge_states\n",
    "        self.num_classes = len(self.charge_states)\n",
    "        \n",
    "        self.classification_types = ['multi_class', 'multi_label']\n",
    "        self.classification_type = classification_type\n",
    "        \n",
    "        self.model_types = ['embedding', 'conv2d', 'prosit']\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        self.df = combine_parquet_into_df(dir_path, file_type)\n",
    "        self.df = drop_na(self.df, 'precursor_intensity')\n",
    "        self.df = keep_desired_charges(self.df)\n",
    "        self.df = aggregate_sequences(self.df)\n",
    "        self.df, self.padding_length = remove_rare_sequence_lengths(self.df)\n",
    "        self.vocabulary, self.voc_len = complete_vocabulary(self.df)\n",
    "        self.df = sequence_encoder(self.df, self.padding_length, self.vocabulary)\n",
    "        self.df = normalize_and_select_most_abundant(self.df)\n",
    "        self.df = encode_charge_states(self.df)\n",
    "        self.df = skip_charges_for_occurrences(self.df)\n",
    "        self.df = get_topK_charge_states(self.df)\n",
    "        print(f\"Step 12/12 complete. Generated dataset with {len(self.df)} sequences.\")\n",
    "        if self.classification_type == 'multi_class':\n",
    "            self.df = self.df[['modified_sequence_vector', 'most_abundant_charge_vector', 'top_2_charge_states']]\n",
    "        elif self.classification_type == 'multi_label':\n",
    "            self.df = self.df[['modified_sequence_vector', 'charge_state_vector', 'top_2_charge_states']]\n",
    "        else:\n",
    "            raise ValueError(\"classification_type must be one of the following: 'multi_class', 'multi_label'\")\n",
    "        \n",
    "        if self.classification_type == \"multi_class\":\n",
    "            if model_type == \"embedding\":\n",
    "                self.data_type = \"tensor\"\n",
    "            elif model_type == \"conv2d\":\n",
    "                self.data_type = \"2d_tensor\"\n",
    "            elif model_type == \"prosit\":\n",
    "                self.data_type = \"tensor\"\n",
    "            else:\n",
    "                raise ValueError(\"model_type must be one of the following: 'embedding', 'conv2d', 'prosit'\")\n",
    "        elif self.classification_type == \"multi_label\":\n",
    "            self.data_type = \"tensor_multi_label\"\n",
    "        else:\n",
    "            raise ValueError(\"classification_type must be one of the following: 'multi_class', 'multi_label'\")\n",
    "        \n",
    "        \n",
    "        self.validation_ratio = 0.2\n",
    "        self.test_mode = True\n",
    "        if test_ratio > 0:\n",
    "            self.test_ratio = test_ratio\n",
    "            self.df_test = self.df.sample(frac = self.test_ratio)\n",
    "            self.test_mode = True\n",
    "        else:\n",
    "            self.df_test = pd.DataFrame()\n",
    "            self.test_mode = False\n",
    "        self.training_validation_df = self.df.drop(self.df_test.index)\n",
    "        self.training_validation_split = StratifiedShuffleSplit(n_splits=1, test_size=self.validation_ratio)\n",
    "        \n",
    "        def create_training_validation_split(df = self.training_validation_df, sssplit = self.training_validation_split):\n",
    "            trainval_ds_embed = np.array(df['modified_sequence_vector']) # TODO\n",
    "            if self.data_type == \"tensor_multi_label\":\n",
    "                trainval_labels_embed = np.array(df['charge_state_vector'])\n",
    "            else:\n",
    "                trainval_labels_embed = np.array(df['most_abundant_charge_vector'])\n",
    "            # Perform the split train and val\n",
    "            train_indicies_embed, val_indicies_embed = next(sssplit.split(trainval_ds_embed, trainval_labels_embed))\n",
    "            # Distribution\n",
    "            train_ds_embed, train_labels_embed = trainval_ds_embed[train_indicies_embed], trainval_labels_embed[train_indicies_embed]\n",
    "            val_ds_embed, val_labels_embed = trainval_ds_embed[val_indicies_embed], trainval_labels_embed[val_indicies_embed]\n",
    "            # create two dataframes for training and validation\n",
    "            if self.data_type == \"tensor_multi_label\":\n",
    "                df_train = pd.DataFrame({'modified_sequence_vector': train_ds_embed, 'charge_state_vector': train_labels_embed})\n",
    "                df_val = pd.DataFrame({'modified_sequence_vector': val_ds_embed, 'charge_state_vector': val_labels_embed})\n",
    "            else:\n",
    "                df_train = pd.DataFrame({'modified_sequence_vector': train_ds_embed, 'most_abundant_charge_vector': train_labels_embed})\n",
    "                df_val = pd.DataFrame({'modified_sequence_vector': val_ds_embed, 'most_abundant_charge_vector': val_labels_embed})\n",
    "            return df_train, df_val\n",
    "        self.df_train, self.df_val = create_training_validation_split(self.training_validation_df, self.training_validation_split)\n",
    "                    \n",
    "        def to_array(df, multi_label=False): \n",
    "            #print(df.head(4))\n",
    "            if multi_label:\n",
    "                label = [np.array(x) for x in df['charge_state_vector']]\n",
    "                data = [np.array(x) for x in df['modified_sequence_vector']]\n",
    "            else:\n",
    "                label = [np.array(x) for x in df['most_abundant_charge_vector']]\n",
    "                data = [np.array(x) for x in df['modified_sequence_vector']]\n",
    "            return label, data\n",
    "        def to_tensor(df, multi_label=False):\n",
    "            label, data = to_array(df, multi_label)\n",
    "            label = tf.convert_to_tensor(label)\n",
    "            data = tf.convert_to_tensor(data)\n",
    "            return label, data\n",
    "        def to_2d_tensor(df):\n",
    "            label, data = to_array(df)\n",
    "            label = tf.convert_to_tensor(label)\n",
    "            data = [np.reshape(np.array(x), (1, self.padding_length, 1)) for x in data]\n",
    "            return label, data\n",
    "        \n",
    "        if self.data_type == \"array\":\n",
    "            self.test_label, self.test_data = to_array(self.df_test)\n",
    "            self.train_label, self.train_data = to_array(self.df_train)\n",
    "            self.val_label, self.val_data = to_array(self.df_val)\n",
    "        elif self.data_type == \"tensor\":\n",
    "            self.test_label, self.test_data = to_tensor(self.df_test)\n",
    "            self.train_label, self.train_data = to_tensor(self.df_train)\n",
    "            self.val_label, self.val_data = to_tensor(self.df_val)\n",
    "        elif self.data_type == \"tensor_multi_label\":\n",
    "            self.test_label, self.test_data = to_tensor(self.df_test, True)\n",
    "            self.train_label, self.train_data = to_tensor(self.df_train, True)\n",
    "            self.val_label, self.val_data = to_tensor(self.df_val, True)\n",
    "        elif self.data_type == \"2d_tensor\":\n",
    "            self.test_label, self.test_data = to_2d_tensor(self.df_test)\n",
    "            self.train_label, self.train_data = to_2d_tensor(self.df_train)\n",
    "            self.val_label, self.val_data = to_2d_tensor(self.df_val)                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:18:59.956723Z",
     "start_time": "2023-09-13T14:16:31.420530300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/12 complete. Combined 12 files into one DataFrame.\n",
      "Step 2/12 complete. Dropped rows with NaN for intensities.\n",
      "Step 3/12 complete. Removed charge states not in [1, 2, 3, 4, 5, 6].\n",
      "Step 4/12 complete. Aggregated all sequences to unique sequences.\n",
      "Step 5/12 complete. Removed 857 of 831677 sequences if sequence-length is represented less than 100 times.\n",
      "Step 6/12 complete. Completed vocabulary with 23 entries.\n",
      "Step 7/12 complete. Encoded all sequences.\n",
      "Step 8/12 complete. Applied normalization, selected most abundant charge state and one-hot encoded it.\n",
      "Step 9/12 complete. Encoded all occuring charge states per unique sequence in a binary vector.\n",
      "Step 10/12 complete. Removed 728 of 830820 sequences if unique charge state distribution is represented less than 1000 times.\n",
      "Step 11/12 complete. Selected top 2 charge states per sequence.\n",
      "Step 12/12 complete. Generated dataset with 830092 sequences.\n"
     ]
    }
   ],
   "source": [
    "my_dataset = PrecursorChargeStateDataset(classification_type=\"multi_class\", model_type=\"embedding\", charge_states=[1, 2, 3, 4, 5, 6], dir_path='data/', file_type='.parquet', columns_to_keep=['modified_sequence','precursor_charge','precursor_intensity'])\n",
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Models\n",
    "\n",
    "General idea:\n",
    "Input: mod_seq_encoded, precursor_charge // precursor_charge_onehot\n",
    "Output: 5 nodes --> highest value == most probable charge for input sequence\n",
    "Use: Softmax, Crossentropy loss\n",
    "\n",
    "stratified split:\n",
    "- PROSITE\n",
    "- CCE\n",
    "- SCCE // ?\n",
    "\n",
    "evaluate models by:\n",
    "- categorical accuracy\n",
    "- f1 score // ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "# imports optimized\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Dropout, Bidirectional, GRU, Conv2D, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from dlomix.layers.attention import AttentionLayer, DecoderAttentionLayer\n",
    "import subprocess\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "class ModelClass:\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.predicted = None\n",
    "        self.prediction = None\n",
    "        self.evaluated = None\n",
    "        self.evaluation = None\n",
    "        self.metrics = None\n",
    "        self.loss = None\n",
    "        self.history = None\n",
    "        self.dataset = dataset\n",
    "        self.num_classes = dataset.num_classes\n",
    "        self.voc_len = dataset.voc_len\n",
    "        self.max_len_seq = dataset.padding_length\n",
    "        self.model_type = dataset.model_type\n",
    "        self.classification_type = dataset.classification_type\n",
    "        self.shape = my_dataset.train_data[0].shape\n",
    "        self.wandb = False\n",
    "        self.compiled = False\n",
    "        self.fitted = False\n",
    "        self.pretrained = False\n",
    "                       \n",
    "        if self.model_type == \"embedding\":\n",
    "            self.model = self.embedding_model()\n",
    "        elif self.model_type == \"conv2d\":\n",
    "            self.model = self.conv2d_model()\n",
    "        elif self.model_type == \"prosit\":\n",
    "            self.model = self.prosit_model()\n",
    "        elif self.model_type == \"multihead\":\n",
    "            self.model = self.multihead_model()\n",
    "        elif self.model_type == \"multilabel\":\n",
    "            self.model = self.multilabel_model()\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be one of the following: 'embedding', 'conv2d', 'prosit', 'multihead', 'multilabel'\")\n",
    "       \n",
    "\n",
    "        \n",
    "    def prosit_model(self):\n",
    "        input_prosit = Input(shape=self.shape)\n",
    "        x = Model(inputs=input_prosit, outputs=input_prosit)\n",
    "        # Embedding, no vocabulary\n",
    "        y = Embedding(input_dim=self.voc_len, output_dim=self.max_len_seq, input_length=self.max_len_seq)(input_prosit)\n",
    "        # Encoder\n",
    "        y = Bidirectional(GRU(256, return_sequences=True))(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        y = GRU(512, return_sequences=True)(y)\n",
    "        y = Dropout(0.5)(y)\n",
    "        # Attention\n",
    "        y = AttentionLayer()(y)\n",
    "        # Regressor\n",
    "        y = Dense(512, activation=\"relu\")(y)\n",
    "        y = Dropout(0.1)(y)\n",
    "        # Output\n",
    "        out = Dense(self.num_classes, activation=\"softmax\")(y)\n",
    "        model_prosit = Model(inputs=[x.input], outputs=out)\n",
    "        return model_prosit\n",
    "    \n",
    "    def conv2d_model(self):\n",
    "        input_convolution = Input(shape=self.shape)\n",
    "        x = Model(inputs=input_convolution, outputs=input_convolution)\n",
    "        y = Rescaling(scale=1./100)(input_convolution)\n",
    "        y = Conv2D(filters=128, kernel_size=(1,3), strides=1, activation=\"relu\", padding='same')(y)\n",
    "        y = Flatten()(y)\n",
    "        y = Dense(210, activation=\"relu\")(y)\n",
    "        z = Dense(self.num_classes, activation=\"softmax\")(y)\n",
    "        model_convolution = Model(inputs=[x.input], outputs=z)\n",
    "        return model_convolution\n",
    "    \n",
    "    def embedding_model(self):\n",
    "        input_embedding = Input(shape=self.shape)\n",
    "        # the first branch operates on the first input\n",
    "        x = Model(inputs=input_embedding, outputs=input_embedding)\n",
    "        y = Embedding(input_dim=self.voc_len, output_dim=self.max_len_seq, input_length=self.max_len_seq)(input_embedding)\n",
    "        y = Flatten()(y)\n",
    "        y = Dense(self.max_len_seq, activation=\"relu\")(y)\n",
    "        z = Dense(self.num_classes, activation=\"softmax\")(y)\n",
    "        model_embed = Model(inputs=[x.input], outputs=z)\n",
    "        return model_embed\n",
    "    \n",
    "    def multihead_model(self):\n",
    "        input_multihead = Input(shape=self.shape)\n",
    "        x = Model(inputs=input_multihead, outputs=input_multihead)\n",
    "        y = Embedding(input_dim=self.voc_len, output_dim=self.max_len_seq, input_length=self.max_len_seq)(input_multihead)\n",
    "        y = Flatten()(y)\n",
    "        branch_outputs = []\n",
    "        for i in range(6):\n",
    "            out = Lambda(lambda x: x[:, i:i+1])(y)   \n",
    "            out = Dense(2, activation=\"sigmoid\")(out)\n",
    "            branch_outputs.append(out)\n",
    "        model_multihead = Model(inputs=[x.input], outputs=branch_outputs)\n",
    "        return model_multihead\n",
    "        \n",
    "    def multilabel_model(self):\n",
    "        input_multilabel = Input(shape=self.shape)\n",
    "        x = Model(inputs=input_multilabel, outputs=input_multilabel)\n",
    "        y = Embedding(input_dim=self.voc_len, output_dim=self.max_len_seq, input_length=self.max_len_seq)(input_multilabel)\n",
    "        y = Flatten()(y)\n",
    "        y = Dense(self.max_len_seq, activation=\"relu\")(y)\n",
    "        z = Dense(self.num_classes, activation=\"sigmoid\")(y)\n",
    "        model_multilabel = Model(inputs=[x.input], outputs=z)\n",
    "        return model_multilabel\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        \n",
    "    \n",
    "    def wandb_init(self, api_key = \"4e8d3dcb1584ad129b3b49ccc34f65b20116ae54\", project_name = \"precursor-charge-state-prediction\" ): # TODO DELETE PRIVATE KEY\n",
    "        subprocess.call(['wandb', 'login', api_key])\n",
    "        wandb.init(project=project_name)\n",
    "        config = wandb.config\n",
    "        config.model_type= self.model_type\n",
    "        config.classification_type= self.classification_type\n",
    "        config.num_classes= self.num_classes\n",
    "        config.voc_len= self.voc_len\n",
    "        config.max_len_seq= self.max_len_seq\n",
    "        self.wandb = True\n",
    "        \n",
    "    def compile(self, lr=0.0001):\n",
    "        if self.classification_type == \"multi_class\":\n",
    "            if self.model == \"prosit\":\n",
    "                self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "                self.loss = 'categorical_crossentropy'\n",
    "                self.metrics = 'categorical_accuracy'\n",
    "            else:\n",
    "                self.model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "                self.loss = 'categorical_crossentropy'\n",
    "                self.metrics = 'categorical_accuracy'\n",
    "                \n",
    "        elif self.classification_type == \"multi_label\":\n",
    "            self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "            self.loss = 'binary_crossentropy'\n",
    "            self.metrics = 'binary_accuracy'\n",
    "        else:\n",
    "            raise ValueError(\"classification_type must be one of the following: 'multi_class', 'multi_label'\")\n",
    "        self.compiled = True\n",
    "        \n",
    "    def fit(self, batch_size=4096, callbacks=None, epochs=30, no_wandb = False):\n",
    "        if not self.compiled:\n",
    "            raise ValueError(\"Model must be compiled before fitting. Use model_class.compile().\")\n",
    "        elif not self.wandb and no_wandb == False:\n",
    "                raise ValueError(\"You did not initialize weights&biases. Set model_class.init(no_wandb=True) or use model_class.wandb_init(api_key= '...', project_name = '...')\")\n",
    "        else:\n",
    "            if callbacks is None:\n",
    "                if no_wandb:\n",
    "                    callbacks = []\n",
    "                else:\n",
    "                    callbacks = [WandbCallback()]\n",
    "            #print(self.shape)\n",
    "            #print(callbacks, len(self.dataset.train_data), len(self.dataset.train_label), len(self.dataset.val_data), len(self.dataset.val_label))\n",
    "            self.history = self.model.fit(self.dataset.train_data, self.dataset.train_label, epochs=epochs, batch_size=batch_size, validation_data=(self.dataset.val_data, self.dataset.val_label), callbacks=callbacks, verbose=1)\n",
    "            \n",
    "            self.fitted = True\n",
    "            \n",
    "    def plot_training(self):\n",
    "        if self.fitted:\n",
    "            # Access the loss, validation loss, and accuracy from the history object\n",
    "            loss = self.history['loss']\n",
    "            val_loss = self.history['val_loss']\n",
    "            accuracy = self.history[self.metrics]\n",
    "            val_accuracy = history.history[self.loss]\n",
    "            \n",
    "            # Plot the loss, validation loss, and accuracy curves\n",
    "            epochs = range(1, len(loss) + 1)\n",
    "            \n",
    "            # Create subplots\n",
    "            fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            \n",
    "            # Plot loss and validation loss\n",
    "            ax1.plot(epochs, loss, 'b', label='Training Loss')\n",
    "            ax1.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "            ax1.set_title('Training and Validation Loss')\n",
    "            ax1.set_xlabel('Epochs')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend()\n",
    "            \n",
    "            # Plot accuracy and validation accuracy\n",
    "            ax2.plot(epochs, accuracy, 'b', label='Training Accuracy')\n",
    "            ax2.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
    "            ax2.set_title('Training and Validation Accuracy')\n",
    "            ax2.set_xlabel('Epochs')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ax2.legend()\n",
    "            \n",
    "            # Adjust spacing between subplots\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Show the plots\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise ValueError(\"Model was not trained. No data to plot. Use model_class.fit()\")\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        self.model = tf.keras.saving.load_model(path)\n",
    "        self.pretrained = True\n",
    "            \n",
    "                \n",
    "    def evaluate(self, test_data=None, test_label=None, test_mode=False):\n",
    "        if not self.fitted:\n",
    "            if not test_mode:\n",
    "                if test_data is None or test_label is None:\n",
    "                    raise ValueError(\"You did not provide test_data and test_label. Use model_class.evaluate(test_data, test_label) or set apply test_ratio>0 to model_class\")\n",
    "                else:\n",
    "                    self.evaluation = self.model.evaluate(test_data, test_label)\n",
    "                    self.evaluated = True\n",
    "            else:\n",
    "                self.evaluation = self.model.evaluate(self.dataset.test_data, self.dataset.test_label)\n",
    "                self.evaluated = True\n",
    "        else:\n",
    "            if self.pretrained:\n",
    "                if test_data is None or test_label is None:\n",
    "                    raise ValueError(\"You did not provide test_data and test_label. Use model_class.evaluate(test_data, test_label) or set apply test_ratio>0 to model_class\")\n",
    "                else:\n",
    "                    self.evaluation = self.model.evaluate(test_data, test_label)\n",
    "                    self.evaluated = True\n",
    "            else:\n",
    "                self.evaluation = self.model.evaluate(self.dataset.test_data, self.dataset.test_label)\n",
    "                self.evaluated = True\n",
    "            \n",
    "        print(f\"test loss, test acc: {self.evaluation}\")\n",
    "            \n",
    "    def predict(self, test_data=None, test_label=None, test_mode=False, no_verification = False):\n",
    "        if not self.fitted:\n",
    "            if not test_mode:\n",
    "                if test_data is None:\n",
    "                    raise ValueError(\"You did not provide test_data and test_label. Use model_class.evaluate(test_data, test_label) or set apply test_ratio>0 to model_class\")\n",
    "                else:\n",
    "                    self.prediction = self.model.predict(test_data)\n",
    "                    self.predicted = True\n",
    "            else:\n",
    "                self.prediction = self.model.predict(test_data)\n",
    "                self.predicted = True\n",
    "        else:\n",
    "            if self.pretrained:\n",
    "                if test_data is None:\n",
    "                    raise ValueError(\"You did not provide test_data and test_label. Use model_class.predict(test_data, test_label) or set apply test_ratio>0 to model_class\")\n",
    "                else:\n",
    "                    self.prediction = self.model.predict(test_data)\n",
    "                    self.predicted = True\n",
    "            else:\n",
    "                test_data = self.dataset.test_data\n",
    "                test_label = self.dataset.test_label\n",
    "                self.prediction = self.model.predict(test_data)\n",
    "                self.predicted = True\n",
    "        \n",
    "        if not no_verification:\n",
    "            if self.classification_type == \"multi_class\":\n",
    "                if test_label is None:\n",
    "                    raise ValueError(\"You did not provide test_label for prediction-verification.\")\n",
    "                else:\n",
    "                    predicted_labels = np.argmax(self.prediction, axis=1)\n",
    "                    true_labels = np.argmax(test_label, axis=1)\n",
    "                        \n",
    "                    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "                    print(\"Accuracy: \", accuracy_score(true_labels, predicted_labels))\n",
    "                    # TODO calculate for score for each class (and/or average)\n",
    "                    # TODO lookup logic for weighted, macro etc. -> presentation\n",
    "                    print(\"Precision_weighted: \", precision_score(true_labels, predicted_labels, average='weighted'))\n",
    "                    print(\"Recall_weighted: \", recall_score(true_labels, predicted_labels, average='weighted'))\n",
    "                    print(\"F1_weighted: \", f1_score(true_labels, predicted_labels, average='weighted'))\n",
    "                    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.dataset.charge_states)\n",
    "                    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n",
    "                    # add legend title and axis labels\n",
    "                    plt.xlabel('Predicted Label')\n",
    "                    plt.ylabel('True Label')\n",
    "                    plt.title('Confusion Matrix')\n",
    "                    # plt.colorbar(label=\"Number of Samples\")\n",
    "                    plt.show()\n",
    "                    \n",
    "                    new_df = pd.DataFrame()\n",
    "                    new_df['charge'] = [1,2,3,4,5,6]\n",
    "                    new_df['precision'] = precision_score(true_labels, predicted_labels, average=None)\n",
    "                    new_df['recall'] = recall_score(true_labels, predicted_labels, average=None)\n",
    "                    new_df['f1'] = f1_score(true_labels, predicted_labels, average=None)\n",
    "                    print(new_df)\n",
    "                   \n",
    "            else:\n",
    "                raise ValueError(\"Not implemented for multi-class.\")    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:30:35.689528400Z",
     "start_time": "2023-09-13T14:30:35.421511900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "model_class = ModelClass(my_dataset)\n",
    "model_class.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "model_class.compile() # TODO callback wandb etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a162c738817443cdab51003a6b0db769"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\micro\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py 837 getcaller\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[175], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwandb_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[172], line 121\u001B[0m, in \u001B[0;36mModelClass.wandb_init\u001B[1;34m(self, api_key, project_name)\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwandb_init\u001B[39m(\u001B[38;5;28mself\u001B[39m, api_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m4e8d3dcb1584ad129b3b49ccc34f65b20116ae54\u001B[39m\u001B[38;5;124m\"\u001B[39m, project_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprecursor-charge-state-prediction\u001B[39m\u001B[38;5;124m\"\u001B[39m ): \u001B[38;5;66;03m# TODO DELETE PRIVATE KEY\u001B[39;00m\n\u001B[0;32m    120\u001B[0m     subprocess\u001B[38;5;241m.\u001B[39mcall([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwandb\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogin\u001B[39m\u001B[38;5;124m'\u001B[39m, api_key])\n\u001B[1;32m--> 121\u001B[0m     \u001B[43mwandb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproject\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproject_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m     config \u001B[38;5;241m=\u001B[39m wandb\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    123\u001B[0m     config\u001B[38;5;241m.\u001B[39mmodel_type\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_type\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1177\u001B[0m, in \u001B[0;36minit\u001B[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001B[0m\n\u001B[0;32m   1175\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m logger\n\u001B[0;32m   1176\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minterrupted\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39me)\n\u001B[1;32m-> 1177\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m   1178\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1179\u001B[0m     error_seen \u001B[38;5;241m=\u001B[39m e\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1154\u001B[0m, in \u001B[0;36minit\u001B[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001B[0m\n\u001B[0;32m   1152\u001B[0m except_exit \u001B[38;5;241m=\u001B[39m wi\u001B[38;5;241m.\u001B[39msettings\u001B[38;5;241m.\u001B[39m_except_exit\n\u001B[0;32m   1153\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1154\u001B[0m     run \u001B[38;5;241m=\u001B[39m \u001B[43mwi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1155\u001B[0m     except_exit \u001B[38;5;241m=\u001B[39m wi\u001B[38;5;241m.\u001B[39msettings\u001B[38;5;241m.\u001B[39m_except_exit\n\u001B[0;32m   1156\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:741\u001B[0m, in \u001B[0;36m_WandbInit.init\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    738\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcommunicating run to backend with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimeout\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m second timeout\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    740\u001B[0m run_init_handle \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39minterface\u001B[38;5;241m.\u001B[39mdeliver_run(run)\n\u001B[1;32m--> 741\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mrun_init_handle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    742\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    743\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_on_progress_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    744\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcancel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    745\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[0;32m    747\u001B[0m     run_result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mrun_result\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:283\u001B[0m, in \u001B[0;36mMailboxHandle.wait\u001B[1;34m(self, timeout, on_probe, on_progress, release, cancel)\u001B[0m\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interface\u001B[38;5;241m.\u001B[39m_transport_keepalive_failed():\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MailboxError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransport failed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 283\u001B[0m found, abandoned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_slot\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_and_clear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwait_timeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m found:\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Always update progress to 100% when done\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m on_progress \u001B[38;5;129;01mand\u001B[39;00m progress_handle \u001B[38;5;129;01mand\u001B[39;00m progress_sent:\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:130\u001B[0m, in \u001B[0;36m_MailboxSlot._get_and_clear\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_and_clear\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Optional[pb\u001B[38;5;241m.\u001B[39mResult], \u001B[38;5;28mbool\u001B[39m]:\n\u001B[0;32m    129\u001B[0m     found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 130\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    131\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    132\u001B[0m             found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "File \u001B[1;32m~\\OneDrive\\Dokumente\\GitHub\\BachelorThesis\\venv\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:126\u001B[0m, in \u001B[0;36m_MailboxSlot._wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:622\u001B[0m, in \u001B[0;36mEvent.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    620\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[0;32m    621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[1;32m--> 622\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    623\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:324\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 324\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    326\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model_class.wandb_init()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:26:21.700327800Z",
     "start_time": "2023-09-13T14:26:16.254895500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "model_class.fit(epochs=1, no_wandb=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2595/2595 [==============================] - 12s 5ms/step - loss: 0.9858 - categorical_accuracy: 0.6072\n",
      "test loss, test acc: [0.9858239889144897, 0.6071991920471191]\n"
     ]
    }
   ],
   "source": [
    "model_class.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:23:55.447833Z",
     "start_time": "2023-09-13T14:23:42.231887300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "model_class.predict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anaylsis / Plots etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-13T10:56:40.372376200Z"
    }
   },
   "outputs": [],
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-13T10:56:40.372376200Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_charge_prediction_text(charge_predictions):\n",
    "    max_charge_index = np.argmax(charge_predictions)\n",
    "    max_charge_value = round(charge_predictions[max_charge_index], 2)\n",
    "\n",
    "    charge_text = f\"The predicted charge state for the input sequence is {max_charge_index+1} [{round(max_charge_value*100,2)}%].\"\n",
    "    percentage_text = \"Prediction percentages for other states:\\n\"\n",
    "\n",
    "    for index, prediction in enumerate(charge_predictions):\n",
    "        if index != max_charge_index:\n",
    "            percentage = round(prediction * 100, 2)\n",
    "            percentage_text += f\"Charge state {index+1}: {percentage}%\\n\"\n",
    "\n",
    "    full_text = charge_text + \"\\n\" + percentage_text\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# Beispiel\n",
    "charge_predictions = np.array([0, 0.3, 0.53, 0.17, 0, 0])\n",
    "output_text = generate_charge_prediction_text(charge_predictions)\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
